<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><title>Day 0 Support of Serving OpenAI GPT-OSS on Hopper and Blackwell GPUs with Free Online Playground | Eigen AI</title><meta name="title" content="Day 0 Support of Serving OpenAI GPT-OSS on Hopper and Blackwell GPUs with Free Online Playground | Eigen AI"/><meta property="og:title" content="Day 0 Support of Serving OpenAI GPT-OSS on Hopper and Blackwell GPUs with Free Online Playground | Eigen AI"/><meta name="twitter:title" content="Day 0 Support of Serving OpenAI GPT-OSS on Hopper and Blackwell GPUs with Free Online Playground | Eigen AI"/><meta name="description" content="&lt;p&gt;In this blog, we&#x27;re excited to achieve day 0 support for the serving of OpenAI’s GPT-OSS-120B and GPT-OSS-20B on Hopper/Blackwell GPUs in collaboration wi..."/><meta property="og:description" content="&lt;p&gt;In this blog, we&#x27;re excited to achieve day 0 support for the serving of OpenAI’s GPT-OSS-120B and GPT-OSS-20B on Hopper/Blackwell GPUs in collaboration wi..."/><meta name="twitter:description" content="&lt;p&gt;In this blog, we&#x27;re excited to achieve day 0 support for the serving of OpenAI’s GPT-OSS-120B and GPT-OSS-20B on Hopper/Blackwell GPUs in collaboration wi..."/><meta property="og:image" content="https://eigenai.com/images/blog/2025-08-09-day0-gpt-oss/arch.png"/><meta name="twitter:image" content="https://eigenai.com/images/blog/2025-08-09-day0-gpt-oss/arch.png"/><meta name="twitter:image:alt" content="The text: Eigen AI. AGI Tomorrow, AEI Today."/><meta property="og:type" content="website"/><meta property="og:url" content="https://eigenai.com/blog/2025-08-09-day0-gpt-oss"/><meta name="twitter:url" content="https://eigenai.com/blog/2025-08-09-day0-gpt-oss"/><meta name="twitter:card" content="summary_large_image"/><meta name="viewport" content="initial-scale=1.0, width=device-width"/><meta name="theme-color" content="#1B3D6D"/><link rel="icon" href="/images/favicon.svg" type="image/svg+xml"/><link rel="preload" href="/fonts/LiberationMono-Bold.ttf" as="font" type="font/ttf" crossorigin="true"/><link rel="preload" href="/fonts/LiberationMono-Regular.ttf" as="font" type="font/ttf" crossorigin="true"/><link rel="preload" href="/fonts/LiberationMono-Italic.ttf" as="font" type="font/ttf" crossorigin="true"/><link rel="preload" href="/fonts/LiberationMono-BoldItalic.ttf" as="font" type="font/ttf" crossorigin="true"/><link rel="preload" href="/fonts/DinishCondensed-Bold.woff" as="font" type="font/woff" crossorigin="true"/><link rel="preload" href="/fonts/DinishCondensed-Bold.woff2" as="font" type="font/woff2" crossorigin="true"/><link rel="preload" href="/fonts/Dinish-Regular.woff" as="font" type="font/woff" crossorigin="true"/><link rel="preload" href="/fonts/Dinish-Regular.woff2" as="font" type="font/woff2" crossorigin="true"/><link rel="preload" href="/fonts/Dinish-Italic.woff" as="font" type="font/woff" crossorigin="true"/><link rel="preload" href="/fonts/Dinish-Italic.woff2" as="font" type="font/woff2" crossorigin="true"/><meta name="next-head-count" content="28"/><link rel="stylesheet" href="/fonts/load.css"/><link rel="preload" href="/_next/static/css/87b868c17796aa2f.css" as="style"/><link rel="stylesheet" href="/_next/static/css/87b868c17796aa2f.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-5cd94c89d3acac5f.js"></script><script src="/_next/static/chunks/webpack-5752944655d749a0.js" defer=""></script><script src="/_next/static/chunks/framework-1f10003e17636e37.js" defer=""></script><script src="/_next/static/chunks/main-6a269cfcb9446759.js" defer=""></script><script src="/_next/static/chunks/pages/_app-73d202999fada4fe.js" defer=""></script><script src="/_next/static/chunks/539-54e9268ed33ffba9.js" defer=""></script><script src="/_next/static/chunks/780-bd3fb50abbee9e93.js" defer=""></script><script src="/_next/static/chunks/pages/%5Bslug%5D-e805b08a323f55f0.js" defer=""></script><script src="/_next/static/Iiz_UNmiYStiB0pKTXmLq/_buildManifest.js" defer=""></script><script src="/_next/static/Iiz_UNmiYStiB0pKTXmLq/_ssgManifest.js" defer=""></script><script src="/_next/static/Iiz_UNmiYStiB0pKTXmLq/_middlewareManifest.js" defer=""></script></head><body><div id="__next"><div class="w-screen full-container flex flex-col"><div class="fixed top-0 left-0 w-full z-30 bg-sky text-paper"><div class="max-w-[830px] mx-auto flex items-center justify-between px-5 py-2"><img src="/images/full_logo.svg" alt="Eigen AI Logo" class="h-8 w-auto cursor-pointer"/><div class="hidden md:flex items-center space-x-6"><div class="flex space-x-4 text-base text-[#1B3D6D]"><a href="https://eigenai.com">About</a><a href="/">Blog</a><a href="https://chat.eigenai.com/">GPT-OSS Playground</a><a href="https://eigenai.com/contact/">Contact</a></div><div class="flex space-x-2"><a href="mailto:hello@eigenai.com" target="_blank" rel="noopener noreferrer" class="text-custom-icon"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"></path></svg></a><a href="https://www.linkedin.com/company/eigen-ai-labs/" target="_blank" rel="noopener noreferrer" class="text-custom-icon"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"></path></svg></a><a href="https://x.com/Eigen_AI_Labs" target="_blank" rel="noopener noreferrer" class="text-custom-icon"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"></path></svg></a><a href="https://github.com/eigen-ai-labs" target="_blank" rel="noopener noreferrer" class="text-custom-icon"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a></div></div><div class="md:hidden"><div><div class="bm-overlay" style="position:fixed;z-index:1000;width:100%;height:100%;background:rgba(0, 0, 0, 0.3);opacity:0;-moz-transform:translate3d(100%, 0, 0);-ms-transform:translate3d(100%, 0, 0);-o-transform:translate3d(100%, 0, 0);-webkit-transform:translate3d(100%, 0, 0);transform:translate3d(100%, 0, 0);transition:opacity 0.3s, transform 0s 0.3s;top:0px;left:0px"></div><div><div class="bm-burger-button" style="z-index:1000;position:fixed;width:1.2em;height:1.0em;right:1.2rem;top:1em"><button type="button" id="react-burger-menu-btn" style="position:absolute;left:0;top:0;z-index:1;width:100%;height:100%;margin:0;padding:0;border:none;font-size:0;background:transparent;cursor:pointer">Open Menu</button><span><span class="bm-burger-bars" style="position:absolute;height:20%;left:0;right:0;top:0%;opacity:1;background:#fff"></span><span class="bm-burger-bars" style="position:absolute;height:20%;left:0;right:0;top:40%;opacity:1;background:#fff"></span><span class="bm-burger-bars" style="position:absolute;height:20%;left:0;right:0;top:80%;opacity:1;background:#fff"></span></span></div></div><div id="" class="bm-menu-wrap" style="position:fixed;right:0;z-index:1100;width:300px;height:100%;-moz-transform:translate3d(100%, 0, 0);-ms-transform:translate3d(100%, 0, 0);-o-transform:translate3d(100%, 0, 0);-webkit-transform:translate3d(100%, 0, 0);transform:translate3d(100%, 0, 0);transition:all 0.5s;top:0px" aria-hidden="true"><div class="bm-menu" style="height:100%;box-sizing:border-box;overflow:auto;background:#1D374E;padding:2.5em 1.5em 0"><nav class="bm-item-list" style="height:100%;color:#fff;padding:0.8em"><div class="bm-item" style="display:inline-block" tabindex="-1"><div class="child:pb-2 child:child:text-2xl"><p><a href="https://eigenai.com">About</a></p><p><a href="/">Blog</a></p><p><a href="https://chat.eigenai.com/">GPT-OSS Chatbot</a></p><p><a href="https://eigenai.com/contact">Contact</a></p></div><div class="child:mr-3 pt-4 child:w-8 child:brightness-100 hover:child:brightness-90 child:transition flex"><a href="mailto:hello@eigenai.com" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"></path></svg></a><a href="https://www.linkedin.com/company/eigen-ai-labs/" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"></path></svg></a><a href="https://x.com/Eigen_AI_Labs" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"></path></svg></a><a href="https://github.com/eigen-ai-labs" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a></div></div></nav></div><div><div class="bm-cross-button" style="position:absolute;width:24px;height:24px;right:8px;top:8px"><button type="button" id="react-burger-cross-btn" style="position:absolute;left:0;top:0;z-index:1;width:100%;height:100%;margin:0;padding:0;border:none;font-size:0;background:transparent;cursor:pointer" tabindex="-1">Close Menu</button><span style="position:absolute;top:6px;right:14px"><span class="bm-cross" style="position:absolute;width:3px;height:14px;transform:rotate(45deg);background:#fff"></span><span class="bm-cross" style="position:absolute;width:3px;height:14px;transform:rotate(-45deg);background:#fff"></span></span></div></div></div></div></div></div></div><div id="content" class="pt-16 md:pt-20 text-paper grow flex child:grow flex-col"><div class="" style="opacity:0"><div class="w-full flex justify-center py-5 pt-16 md:pt-5"><div class="container px-5" lang="en"><h1 lang="en" class="text-3xl md:text-3xl w-full font-bold title-flow">Day 0 Support of Serving OpenAI GPT-OSS on Hopper and Blackwell GPUs with Free Online Playground</h1><p class="text-lg pt-2 pb-2">by: <!-- -->Eigen AI Team<!-- -->,<!-- --> <!-- -->09 Aug, 2025<!-- --></p><hr/><div class="pt-2 article"><p>In this blog, we're excited to achieve day 0 support for the serving of OpenAI’s GPT-OSS-120B and GPT-OSS-20B on Hopper/Blackwell GPUs in collaboration with the SGLang team. We also introduce a free online playground featuring both web and API access to the GPT-OSS-120B model (<a href="https://chat.eigenai.com">chat.eigenai.com</a>) in partnership with Yotta Labs. Major performance optimizations are already in the pipeline, and we expect to offer even faster, smarter, and more scalable services in the days ahead.</p>
<hr>
<h2><a id="introduction-of-gpt-oss-architecture" class="anchor" href="#introduction-of-gpt-oss-architecture" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Introduction of GPT-OSS Architecture</h2>
<p>GPT-OSS is a newly released open-source large language model (LLM) from OpenAI designed for the efficient deployment of reasoning tasks. OpenAI open-sourced two versions of GPT-OSS: <a href="https://huggingface.co/openai/gpt-oss-20b/blob/main/README.md">GPT-OSS-120B and GPT-OSS-20B</a>.</p>
<ul>
<li><strong>GPT-OSS-120B</strong>: for production, general purpose, high reasoning use cases (117B parameters)</li>
<li><strong>GPT-OSS-20B</strong>: for lower latency, and local or specialized use cases (21B parameters)</li>
</ul>
<p>Figure 1 illustrates the architectural details.</p>
<p><img src="/images/blog/2025-08-09-day0-gpt-oss/arch.png" alt="GPT-OSS Architecture"></p>
<p><strong>Figure 1.</strong> Illustration of GPT-OSS architecture. GptOssMoE layers and GptOssAttn (sinked sliding window grouped-query attention) layers.
<br><br></p>
<hr>
<h3><a id="gptossattn" class="anchor" href="#gptossattn" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>GptOssAttn</h3>
<p>GPT-OSS introduces a novel attention mechanism that interleaves sliding window attention and full attention. Specifically, odd-numbered layers use sliding window grouped-query attention (GQA) with a window size of 128, and even layers use full attention. All attention layers include bias terms in the linear projection layers (queries, keys, values, and output). A distinctive sinked attention mechanism is applied: for each attention head, a learnable scalar parameter (referred to as the sink) is appended as the last column of the attention score matrix <strong>S</strong> prior to softmax. After softmax computation, the sink column is detached, yielding a standard square attention matrix <strong>A</strong>. This operation is mathematically equivalent to modifying the softmax denominator with an additional exponential term.</p>
<p><img src="/images/blog/2025-08-09-day0-gpt-oss/eqn1.png"
     alt="Attention Sink Mechanism"
     style="width:200px; height:auto;"></p>
<p><strong>Why Attention Sinks?</strong> Adding just one learnable parameter per attention head enables the model to &quot;pay no attention to any tokens&quot; when needed. The presence of a sink draws attention away from other tokens, limiting the spread of information (and noise) and resulting in more stable embeddings, which is especially critical for the stability of sliding window attention. OpenAI's model card for GPT-OSS-20B explains the attention sink mechanism, directly connecting the design to our research, i.e., StreamingLLM [1] and SpAtten [2].</p>
<p><img src="/images/blog/2025-08-09-day0-gpt-oss/streamingllm.png" alt="StreamingLLM Figure">
<strong>Figure 2.</strong> In our StreamingLLM [1], the first few &quot;sink&quot; tokens are kept in the cache, which has unlocked infinite-length generation by respecting the model's existing attention patterns.</p>
<p>Our approach in StreamingLLM learns a full token embedding that can interact differently with each query (see Figure 2), while OpenAI instead adds a single learnable value that acts as a universal escape valve. Both successfully solve the fundamental problem: giving attention somewhere to go when it has nothing meaningful to attend to.</p>
<p><img src="/images/blog/2025-08-09-day0-gpt-oss/spatten.png" alt="SpAtten Figure">
<strong>Figure 3.</strong> Initial tokens are always kept in SpAtten [2].
<br><br></p>
<p>A similar phenomenon has been observed in BERT, where &quot;a surprisingly large amount of attention focuses on the delimiter token [SEP] and periods. Our work SpAtten [2] in 2021, which pioneered attention score-based KV Cache pruning and quantization of attention, also found that GPT-2 preserved its initial token, as shown in Figure 3.</p>
<p>The attention sink mechanism we discovered [1,2] has since been adopted across the industry, appearing in production systems like OpenAI's models and inspiring new research directions in quantization and model optimization.
<br><br></p>
<hr>
<h3><a id="gptossmoe" class="anchor" href="#gptossmoe" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>GptOssMoE</h3>
<p>For Mixture-of-Experts (MoE) layers, each token selects the top-4 experts based on router scores from 128 (for 120B) or 32 (for 20B) experts. The expert adopts <strong>SwigLU</strong> activation, and three linear layers, including gate, up, and down projections, all have bias terms enabled. The gate branch employs a <strong>Swish</strong> activation function with β = 1.702, and the up branch is added by 1 and multiplied with the gate output. To ensure numerical stability:</p>
<ul>
<li>The gate pre-activation is clipped at a maximum value of 7 before applying Swish.</li>
<li>The up branch is clipped to the range [−7, 7] before the +1 shift.</li>
</ul>
<p>The MoE computation is summarized as:</p>
<img src="/images/blog/2025-08-09-day0-gpt-oss/eqn2.png" alt="MoE Computation" style="width:400px; height:auto;">
<p>The outputs of the top-k selected experts will be summed up with the softmax-normalized router weights.</p>
<img src="/images/blog/2025-08-09-day0-gpt-oss/eqn3.png" alt="MoE Output" style="width:300px; height:auto;">
<hr>
<h2><a id="try-local-gpt-oss-serving-on-hopperblackwell-gpus" class="anchor" href="#try-local-gpt-oss-serving-on-hopperblackwell-gpus" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Try Local GPT-OSS Serving on Hopper/Blackwell GPUs</h2>
<p>Please follow the instructions <a href="https://github.com/sgl-project/sglang/issues/8833">here</a> to set up a ready-to-use container to try out GPT-OSS serving.
<br><br></p>
<hr>
<h2><a id="introducing-the-eigen-ai-playground" class="anchor" href="#introducing-the-eigen-ai-playground" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Introducing the Eigen AI Playground</h2>
<p>We’re thrilled to announce the launch of the <strong>Eigen AI Playground</strong>, built in partnership with Yotta Labs – a free online platform where anyone can experience the power of OpenAI’s GPT-OSS-120B model right in their browser: <a href="https://chat.eigenai.com">chat.eigenai.com</a>. This platform is well optimized for performance and includes comprehensive, reproducible deployment guides, enabling users to integrate these advancements into their projects with ease. This initiative reflects our broader vision to make the transformative power of AI accessible globally, fostering innovation across industries and communities.</p>
<p><strong>Explore now on the web:</strong></p>
<p>The web interface offers an intuitive and smooth chat experience with GPT-OSS-120B:</p>
<img src="/images/blog/2025-08-09-day0-gpt-oss/chatbot.png" alt="Web Playground" style="width:100%">
<p><strong>Integrate via API:</strong></p>
<p>For developers or advanced users, we also provide a simple API that allows you to integrate GPT-OSS-120B directly into your applications for free.</p>
<pre><code class="language-python"># First install: pip install openai

import openai

client = openai.OpenAI(
    api_key=&quot;xxx&quot;,
    base_url=&quot;https://gateway.eigenai.com/inference/public/v1&quot;
)

response = client.chat.completions.create(
    model=&quot;OpenAI/GPT-OSS-120B&quot;,
    messages=[
        {&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: &quot;what can i help you?&quot;},
        {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;test&quot;},
    ],
    temperature=0.6,
    max_tokens=8192,
    stream=False,
)

# Print the completion result
print(response.choices[0].message.content)
</code></pre>
<p><br><br></p>
<hr>
<h2><a id="acknowledgments" class="anchor" href="#acknowledgments" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Acknowledgments</h2>
<p>We would like to express our heartfelt gratitude to the following teams and collaborators:</p>
<ul>
<li><strong>Eigen AI Team &amp; Community</strong> – Jinglei Cheng, Jiaqi Gu, Yipin Guo, Di Jin, Yilian Uill Liu, Shuqing Luo, Zilin Shen, Ryan Wang, Wei-Chen Wang, Genghan Zhang, and many others.</li>
<li><a href="https://github.com/sgl-project/sglang">SGLang Community</a></li>
<li><a href="https://www.yottalabs.ai/">Yotta Labs</a></li>
<li><a href="https://innomatrix.ai/">Innomatrix</a><br>
<br><br></li>
</ul>
<hr>
<h2><a id="further-reading" class="anchor" href="#further-reading" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Further Reading</h2>
<ul>
<li><a href="https://x.com/Eigen_AI_Labs/status/1952931721655292056">At Eigen AI, we’re on a mission to make sure the AI revolution doesn’t leave anyone behind. | Eigen AI</a></li>
<li><a href="https://github.com/sgl-project/sglang">SGLang GitHub Repository</a></li>
<li><a href="https://huggingface.co/eigen-ai-labs/gpt-oss-120b-bf16">EigenAI Labs Huggingface Repository with GPT-OSS BF16 Checkpoints</a></li>
<li><a href="https://hanlab.mit.edu/blog/streamingllm">How Attention Sinks Keep Language Models Stable</a></li>
<li><a href="https://cdn.openai.com/pdf/419b6906-9da6-406c-a19d-1bb078ac7637/oai_gpt-oss_model_card.pdf">OpenAI GPT-OSS model card</a><br>
<br><br></li>
</ul>
<hr>
<h2><a id="references" class="anchor" href="#references" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>References</h2>
<p>[1] Xiao, G., Tian, Y., Chen, B., Han, S., &amp; Lewis, M., “Efficient Streaming Language Models with Attention Sinks,” ICLR 2024.<br>
[2] Wang, H., Zhang, Z., &amp; Han, S., “SpAtten: Efficient Sparse Attention Architecture with Cascade Token and Head Pruning,” HPCA 2021.
<br><br></p>
</div></div></div></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"frontmatter":{"title":"Day 0 Support of Serving OpenAI GPT-OSS on Hopper and Blackwell GPUs with Free Online Playground","author":"Eigen AI Team","date":"Aug 09 2025","previewImg":"/images/blog/2025-08-09-day0-gpt-oss/arch.png"},"content":"\nIn this blog, we're excited to achieve day 0 support for the serving of OpenAI’s GPT-OSS-120B and GPT-OSS-20B on Hopper/Blackwell GPUs in collaboration with the SGLang team. We also introduce a free online playground featuring both web and API access to the GPT-OSS-120B model ([chat.eigenai.com](https://chat.eigenai.com)) in partnership with Yotta Labs. Major performance optimizations are already in the pipeline, and we expect to offer even faster, smarter, and more scalable services in the days ahead.\n\n---\n\n## Introduction of GPT-OSS Architecture\n\nGPT-OSS is a newly released open-source large language model (LLM) from OpenAI designed for the efficient deployment of reasoning tasks. OpenAI open-sourced two versions of GPT-OSS: [GPT-OSS-120B and GPT-OSS-20B](https://huggingface.co/openai/gpt-oss-20b/blob/main/README.md).\n\n- **GPT-OSS-120B**: for production, general purpose, high reasoning use cases (117B parameters)  \n- **GPT-OSS-20B**: for lower latency, and local or specialized use cases (21B parameters)  \n\nFigure 1 illustrates the architectural details.\n\n![GPT-OSS Architecture](/images/blog/2025-08-09-day0-gpt-oss/arch.png)\n\n**Figure 1.** Illustration of GPT-OSS architecture. GptOssMoE layers and GptOssAttn (sinked sliding window grouped-query attention) layers.\n\u003cbr\u003e\u003cbr\u003e\n\n---\n\n### GptOssAttn\n\nGPT-OSS introduces a novel attention mechanism that interleaves sliding window attention and full attention. Specifically, odd-numbered layers use sliding window grouped-query attention (GQA) with a window size of 128, and even layers use full attention. All attention layers include bias terms in the linear projection layers (queries, keys, values, and output). A distinctive sinked attention mechanism is applied: for each attention head, a learnable scalar parameter (referred to as the sink) is appended as the last column of the attention score matrix **S** prior to softmax. After softmax computation, the sink column is detached, yielding a standard square attention matrix **A**. This operation is mathematically equivalent to modifying the softmax denominator with an additional exponential term.\n\n\u003cimg src=\"/images/blog/2025-08-09-day0-gpt-oss/eqn1.png\"\n     alt=\"Attention Sink Mechanism\"\n     style=\"width:200px; height:auto;\"\u003e\n\n**Why Attention Sinks?** Adding just one learnable parameter per attention head enables the model to \"pay no attention to any tokens\" when needed. The presence of a sink draws attention away from other tokens, limiting the spread of information (and noise) and resulting in more stable embeddings, which is especially critical for the stability of sliding window attention. OpenAI's model card for GPT-OSS-20B explains the attention sink mechanism, directly connecting the design to our research, i.e., StreamingLLM [1] and SpAtten [2].\n\n![StreamingLLM Figure](/images/blog/2025-08-09-day0-gpt-oss/streamingllm.png)\n**Figure 2.** In our StreamingLLM [1], the first few \"sink\" tokens are kept in the cache, which has unlocked infinite-length generation by respecting the model's existing attention patterns.\n\nOur approach in StreamingLLM learns a full token embedding that can interact differently with each query (see Figure 2), while OpenAI instead adds a single learnable value that acts as a universal escape valve. Both successfully solve the fundamental problem: giving attention somewhere to go when it has nothing meaningful to attend to.\n\n![SpAtten Figure](/images/blog/2025-08-09-day0-gpt-oss/spatten.png)\n**Figure 3.** Initial tokens are always kept in SpAtten [2].\n\u003cbr\u003e\u003cbr\u003e\n\nA similar phenomenon has been observed in BERT, where \"a surprisingly large amount of attention focuses on the delimiter token [SEP] and periods. Our work SpAtten [2] in 2021, which pioneered attention score-based KV Cache pruning and quantization of attention, also found that GPT-2 preserved its initial token, as shown in Figure 3.\n\nThe attention sink mechanism we discovered [1,2] has since been adopted across the industry, appearing in production systems like OpenAI's models and inspiring new research directions in quantization and model optimization.\n\u003cbr\u003e\u003cbr\u003e\n\n---\n\n### GptOssMoE\n\nFor Mixture-of-Experts (MoE) layers, each token selects the top-4 experts based on router scores from 128 (for 120B) or 32 (for 20B) experts. The expert adopts **SwigLU** activation, and three linear layers, including gate, up, and down projections, all have bias terms enabled. The gate branch employs a **Swish** activation function with β = 1.702, and the up branch is added by 1 and multiplied with the gate output. To ensure numerical stability:\n\n- The gate pre-activation is clipped at a maximum value of 7 before applying Swish.  \n- The up branch is clipped to the range [−7, 7] before the +1 shift.\n\nThe MoE computation is summarized as:\n\n\u003cimg src=\"/images/blog/2025-08-09-day0-gpt-oss/eqn2.png\" alt=\"MoE Computation\" style=\"width:400px; height:auto;\"\u003e\n\nThe outputs of the top-k selected experts will be summed up with the softmax-normalized router weights.\n\n\u003cimg src=\"/images/blog/2025-08-09-day0-gpt-oss/eqn3.png\" alt=\"MoE Output\" style=\"width:300px; height:auto;\"\u003e\n\n---\n\n## Try Local GPT-OSS Serving on Hopper/Blackwell GPUs\n\nPlease follow the instructions [here](https://github.com/sgl-project/sglang/issues/8833) to set up a ready-to-use container to try out GPT-OSS serving.\n\u003cbr\u003e\u003cbr\u003e\n\n---\n\n## Introducing the Eigen AI Playground\n\nWe’re thrilled to announce the launch of the **Eigen AI Playground**, built in partnership with Yotta Labs – a free online platform where anyone can experience the power of OpenAI’s GPT-OSS-120B model right in their browser: [chat.eigenai.com](https://chat.eigenai.com). This platform is well optimized for performance and includes comprehensive, reproducible deployment guides, enabling users to integrate these advancements into their projects with ease. This initiative reflects our broader vision to make the transformative power of AI accessible globally, fostering innovation across industries and communities.\n\n**Explore now on the web:**\n\nThe web interface offers an intuitive and smooth chat experience with GPT-OSS-120B:  \n\n\u003cimg src=\"/images/blog/2025-08-09-day0-gpt-oss/chatbot.png\" alt=\"Web Playground\" style=\"width:100%\"\u003e\n\n**Integrate via API:**\n\nFor developers or advanced users, we also provide a simple API that allows you to integrate GPT-OSS-120B directly into your applications for free.\n\n```python\n# First install: pip install openai\n\nimport openai\n\nclient = openai.OpenAI(\n    api_key=\"xxx\",\n    base_url=\"https://gateway.eigenai.com/inference/public/v1\"\n)\n\nresponse = client.chat.completions.create(\n    model=\"OpenAI/GPT-OSS-120B\",\n    messages=[\n        {\"role\": \"assistant\", \"content\": \"what can i help you?\"},\n        {\"role\": \"user\", \"content\": \"test\"},\n    ],\n    temperature=0.6,\n    max_tokens=8192,\n    stream=False,\n)\n\n# Print the completion result\nprint(response.choices[0].message.content)\n```\n\u003cbr\u003e\u003cbr\u003e\n\n---\n## Acknowledgments\n\nWe would like to express our heartfelt gratitude to the following teams and collaborators:\n\n- **Eigen AI Team \u0026 Community** – Jinglei Cheng, Jiaqi Gu, Yipin Guo, Di Jin, Yilian Uill Liu, Shuqing Luo, Zilin Shen, Ryan Wang, Wei-Chen Wang, Genghan Zhang, and many others.  \n- [SGLang Community](https://github.com/sgl-project/sglang)  \n- [Yotta Labs](https://www.yottalabs.ai/)  \n- [Innomatrix](https://innomatrix.ai/)  \n\u003cbr\u003e\u003cbr\u003e\n\n---\n\n## Further Reading\n\n- [At Eigen AI, we’re on a mission to make sure the AI revolution doesn’t leave anyone behind. | Eigen AI](https://x.com/Eigen_AI_Labs/status/1952931721655292056)  \n- [SGLang GitHub Repository](https://github.com/sgl-project/sglang)  \n- [EigenAI Labs Huggingface Repository with GPT-OSS BF16 Checkpoints](https://huggingface.co/eigen-ai-labs/gpt-oss-120b-bf16)  \n- [How Attention Sinks Keep Language Models Stable](https://hanlab.mit.edu/blog/streamingllm)  \n- [OpenAI GPT-OSS model card](https://cdn.openai.com/pdf/419b6906-9da6-406c-a19d-1bb078ac7637/oai_gpt-oss_model_card.pdf)  \n\u003cbr\u003e\u003cbr\u003e\n\n---\n\n## References\n\n[1] Xiao, G., Tian, Y., Chen, B., Han, S., \u0026 Lewis, M., “Efficient Streaming Language Models with Attention Sinks,” ICLR 2024.  \n[2] Wang, H., Zhang, Z., \u0026 Han, S., “SpAtten: Efficient Sparse Attention Architecture with Cascade Token and Head Pruning,” HPCA 2021.\n\u003cbr\u003e\u003cbr\u003e\n","slug":"2025-08-09-day0-gpt-oss"},"__N_SSG":true},"page":"/[slug]","query":{"slug":"2025-08-09-day0-gpt-oss"},"buildId":"Iiz_UNmiYStiB0pKTXmLq","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>